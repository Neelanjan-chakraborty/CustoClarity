{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb768a53",
   "metadata": {},
   "source": [
    "# CUSTO CLARITY - Customer Segmentation Analysis\n",
    "## 01. Exploratory Data Analysis\n",
    "\n",
    "**Author**: Neelanjan Chakraborty  \n",
    "**Website**: [neelanjanchakraborty.in](https://neelanjanchakraborty.in/)  \n",
    "**Project**: Customer Segmentation for Retail Strategy  \n",
    "\n",
    "---\n",
    "\n",
    "### 📋 Project Overview\n",
    "\n",
    "This notebook performs comprehensive Exploratory Data Analysis (EDA) on the Mall Customer Segmentation dataset to understand customer demographics, spending patterns, and identify potential segments for targeted marketing strategies.\n",
    "\n",
    "### 🎯 Objectives\n",
    "- Understand the structure and quality of customer data\n",
    "- Analyze customer demographics and spending behavior\n",
    "- Identify patterns and correlations in the data\n",
    "- Prepare insights for clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6b236d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import libraries\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path for importing custom modules\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "# Import custom modules\n",
    "from data_loader import DataLoader\n",
    "from visualizer import CustomerVisualizationSuite\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")\n",
    "print(f\"📊 Pandas version: {pd.__version__}\")\n",
    "print(f\"🔢 NumPy version: {np.__version__}\")\n",
    "print(f\"📈 Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"🎨 Seaborn version: {sns.__version__}\")\n",
    "print(\"\\n🚀 Ready for Customer Segmentation Analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f993e798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "loader = DataLoader()\n",
    "df = loader.load_dataset()\n",
    "\n",
    "print(\"📊 DATASET LOADED SUCCESSFULLY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(\"\\n📋 First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe801a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Information Analysis\n",
    "print(\"🔍 COMPREHENSIVE DATASET ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic information\n",
    "print(f\"📊 Dataset Dimensions: {df.shape[0]} rows × {df.shape[1]} columns\")\n",
    "print(f\"💾 Memory Usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "print(\"\\n📈 Column Information:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n📊 Statistical Summary:\")\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "print(\"\\n🔍 Data Types:\")\n",
    "for col, dtype in df.dtypes.items():\n",
    "    print(f\"  {col}: {dtype}\")\n",
    "\n",
    "print(\"\\n❓ Missing Values:\")\n",
    "missing_values = df.isnull().sum()\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"  ✅ No missing values found!\")\n",
    "else:\n",
    "    for col, missing in missing_values.items():\n",
    "        if missing > 0:\n",
    "            print(f\"  {col}: {missing} ({missing/len(df)*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n🔄 Duplicate Rows:\")\n",
    "duplicates = df.duplicated().sum()\n",
    "if duplicates == 0:\n",
    "    print(\"  ✅ No duplicate rows found!\")\n",
    "else:\n",
    "    print(f\"  ⚠️ Found {duplicates} duplicate rows\")\n",
    "\n",
    "print(\"\\n🎯 Unique Values per Column:\")\n",
    "for col in df.columns:\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"  {col}: {unique_count} unique values\")\n",
    "    if unique_count <= 10 and df[col].dtype == 'object':\n",
    "        print(f\"    Values: {list(df[col].unique())}\")\n",
    "    elif df[col].dtype in ['int64', 'float64']:\n",
    "        print(f\"    Range: {df[col].min()} - {df[col].max()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93807d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Visualization Suite\n",
    "viz_suite = CustomerVisualizationSuite(figsize=(15, 10))\n",
    "\n",
    "print(\"🎨 CREATING COMPREHENSIVE DATA VISUALIZATIONS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create data overview plots\n",
    "viz_suite.plot_data_overview(df)\n",
    "\n",
    "print(\"✅ Data overview visualization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de809c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "print(\"🔗 CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Create correlation matrix\n",
    "viz_suite.plot_correlation_matrix(df)\n",
    "\n",
    "# Calculate and display correlation values\n",
    "numeric_df = df.select_dtypes(include=[np.number])\n",
    "if 'CustomerID' in numeric_df.columns:\n",
    "    numeric_df = numeric_df.drop('CustomerID', axis=1)\n",
    "\n",
    "correlation_matrix = numeric_df.corr()\n",
    "print(\"📊 Correlation Matrix:\")\n",
    "print(correlation_matrix.round(3))\n",
    "\n",
    "# Find strongest correlations\n",
    "correlations = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        col1, col2 = correlation_matrix.columns[i], correlation_matrix.columns[j]\n",
    "        corr_value = correlation_matrix.iloc[i, j]\n",
    "        correlations.append((col1, col2, abs(corr_value), corr_value))\n",
    "\n",
    "correlations.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"\\n🔥 Strongest Correlations:\")\n",
    "for col1, col2, abs_corr, corr in correlations[:5]:\n",
    "    direction = \"positive\" if corr > 0 else \"negative\"\n",
    "    strength = \"very strong\" if abs_corr > 0.8 else \"strong\" if abs_corr > 0.6 else \"moderate\" if abs_corr > 0.4 else \"weak\"\n",
    "    print(f\"  {col1} ↔ {col2}: {corr:.3f} ({strength} {direction})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935d4bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier Analysis\n",
    "print(\"🚨 OUTLIER DETECTION ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create outlier visualization\n",
    "viz_suite.plot_outlier_analysis(df)\n",
    "\n",
    "# Detect outliers using IQR method\n",
    "from preprocessor import CustomerDataPreprocessor\n",
    "preprocessor = CustomerDataPreprocessor()\n",
    "\n",
    "numeric_cols = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n",
    "outliers = preprocessor.detect_outliers(df, numeric_cols, method='iqr', threshold=1.5)\n",
    "\n",
    "print(\"📊 Outlier Detection Results (IQR Method):\")\n",
    "for col, outlier_indices in outliers.items():\n",
    "    if len(outlier_indices) > 0:\n",
    "        print(f\"\\n📍 {col}:\")\n",
    "        print(f\"  • Number of outliers: {len(outlier_indices)}\")\n",
    "        print(f\"  • Percentage of data: {len(outlier_indices)/len(df)*100:.2f}%\")\n",
    "        outlier_values = df.loc[outlier_indices, col].values\n",
    "        print(f\"  • Outlier range: {outlier_values.min():.1f} - {outlier_values.max():.1f}\")\n",
    "        print(f\"  • Normal range (Q1-Q3): {df[col].quantile(0.25):.1f} - {df[col].quantile(0.75):.1f}\")\n",
    "    else:\n",
    "        print(f\"\\n✅ {col}: No outliers detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cd195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Demographics Analysis\n",
    "print(\"👥 CUSTOMER DEMOGRAPHICS ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Age Analysis\n",
    "print(\"🎂 Age Demographics:\")\n",
    "print(f\"  • Average Age: {df['Age'].mean():.1f} years\")\n",
    "print(f\"  • Age Range: {df['Age'].min()} - {df['Age'].max()} years\")\n",
    "print(f\"  • Most Common Age: {df['Age'].mode().values[0]} years\")\n",
    "\n",
    "age_groups = pd.cut(df['Age'], bins=[0, 25, 35, 50, 100], labels=['Young (18-25)', 'Adult (26-35)', 'Middle-aged (36-50)', 'Senior (50+)'])\n",
    "age_distribution = age_groups.value_counts()\n",
    "print(f\"\\n📊 Age Group Distribution:\")\n",
    "for group, count in age_distribution.items():\n",
    "    percentage = count / len(df) * 100\n",
    "    print(f\"  • {group}: {count} customers ({percentage:.1f}%)\")\n",
    "\n",
    "# Gender Analysis\n",
    "print(f\"\\n⚧ Gender Demographics:\")\n",
    "gender_distribution = df['Gender'].value_counts()\n",
    "for gender, count in gender_distribution.items():\n",
    "    percentage = count / len(df) * 100\n",
    "    print(f\"  • {gender}: {count} customers ({percentage:.1f}%)\")\n",
    "\n",
    "# Income Analysis\n",
    "print(f\"\\n💰 Income Demographics:\")\n",
    "print(f\"  • Average Income: ${df['Annual Income (k$)'].mean():.1f}k\")\n",
    "print(f\"  • Income Range: ${df['Annual Income (k$)'].min()}k - ${df['Annual Income (k$)'].max()}k\")\n",
    "print(f\"  • Median Income: ${df['Annual Income (k$)'].median():.1f}k\")\n",
    "\n",
    "income_groups = pd.cut(df['Annual Income (k$)'], bins=[0, 40, 70, 200], labels=['Low Income (<$40k)', 'Medium Income ($40-70k)', 'High Income (>$70k)'])\n",
    "income_distribution = income_groups.value_counts()\n",
    "print(f\"\\n📊 Income Group Distribution:\")\n",
    "for group, count in income_distribution.items():\n",
    "    percentage = count / len(df) * 100\n",
    "    print(f\"  • {group}: {count} customers ({percentage:.1f}%)\")\n",
    "\n",
    "# Spending Analysis\n",
    "print(f\"\\n💳 Spending Score Demographics:\")\n",
    "print(f\"  • Average Spending Score: {df['Spending Score (1-100)'].mean():.1f}/100\")\n",
    "print(f\"  • Spending Range: {df['Spending Score (1-100)'].min()} - {df['Spending Score (1-100)'].max()}\")\n",
    "print(f\"  • Median Spending Score: {df['Spending Score (1-100)'].median():.1f}/100\")\n",
    "\n",
    "spending_groups = pd.cut(df['Spending Score (1-100)'], bins=[0, 35, 65, 100], labels=['Low Spender (1-35)', 'Medium Spender (36-65)', 'High Spender (66-100)'])\n",
    "spending_distribution = spending_groups.value_counts()\n",
    "print(f\"\\n📊 Spending Group Distribution:\")\n",
    "for group, count in spending_distribution.items():\n",
    "    percentage = count / len(df) * 100\n",
    "    print(f\"  • {group}: {count} customers ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed292dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Customer Behavior Analysis\n",
    "print(\"🎯 ADVANCED CUSTOMER BEHAVIOR ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Gender-based Analysis\n",
    "print(\"⚧ Gender-based Spending Patterns:\")\n",
    "gender_stats = df.groupby('Gender')[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']].agg(['mean', 'std', 'median']).round(2)\n",
    "print(gender_stats)\n",
    "\n",
    "# Create gender comparison visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "fig.suptitle('Customer Behavior by Gender\\nCUSTO CLARITY - by Neelanjan Chakraborty', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Age by Gender\n",
    "sns.boxplot(data=df, x='Gender', y='Age', ax=axes[0])\n",
    "axes[0].set_title('Age Distribution by Gender')\n",
    "\n",
    "# Income by Gender\n",
    "sns.boxplot(data=df, x='Gender', y='Annual Income (k$)', ax=axes[1])\n",
    "axes[1].set_title('Income Distribution by Gender')\n",
    "\n",
    "# Spending by Gender\n",
    "sns.boxplot(data=df, x='Gender', y='Spending Score (1-100)', ax=axes[2])\n",
    "axes[2].set_title('Spending Score by Gender')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Age vs Income vs Spending Analysis\n",
    "print(f\"\\n🔍 Age-Income-Spending Relationships:\")\n",
    "\n",
    "# Create age groups for analysis\n",
    "df_analysis = df.copy()\n",
    "df_analysis['Age_Group'] = age_groups\n",
    "df_analysis['Income_Group'] = income_groups\n",
    "df_analysis['Spending_Group'] = spending_groups\n",
    "\n",
    "# Cross-tabulation analysis\n",
    "print(\"\\n📊 Age Group vs Income Group:\")\n",
    "age_income_crosstab = pd.crosstab(df_analysis['Age_Group'], df_analysis['Income_Group'], margins=True)\n",
    "print(age_income_crosstab)\n",
    "\n",
    "print(\"\\n📊 Income Group vs Spending Group:\")\n",
    "income_spending_crosstab = pd.crosstab(df_analysis['Income_Group'], df_analysis['Spending_Group'], margins=True)\n",
    "print(income_spending_crosstab)\n",
    "\n",
    "# Interactive 3D Scatter Plot\n",
    "print(\"\\n🎨 Creating Interactive 3D Customer Visualization...\")\n",
    "fig_3d = px.scatter_3d(df, x='Age', y='Annual Income (k$)', z='Spending Score (1-100)',\n",
    "                      color='Gender', size_max=18, size='Annual Income (k$)',\n",
    "                      title='3D Customer Segmentation View<br>CUSTO CLARITY - by Neelanjan Chakraborty',\n",
    "                      labels={'Annual Income (k$)': 'Annual Income ($k)',\n",
    "                             'Spending Score (1-100)': 'Spending Score'})\n",
    "\n",
    "fig_3d.update_layout(scene=dict(\n",
    "    xaxis_title='Age (years)',\n",
    "    yaxis_title='Annual Income ($k)',\n",
    "    zaxis_title='Spending Score (1-100)'\n",
    "))\n",
    "\n",
    "fig_3d.show()\n",
    "\n",
    "print(\"✅ Advanced customer behavior analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ffb736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Quality Assessment\n",
    "print(\"✅ DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Calculate data quality metrics\n",
    "total_records = len(df)\n",
    "complete_records = df.dropna().shape[0]\n",
    "data_completeness = complete_records / total_records * 100\n",
    "\n",
    "print(f\"📊 Data Quality Metrics:\")\n",
    "print(f\"  • Total Records: {total_records:,}\")\n",
    "print(f\"  • Complete Records: {complete_records:,}\")\n",
    "print(f\"  • Data Completeness: {data_completeness:.1f}%\")\n",
    "print(f\"  • Duplicate Records: {df.duplicated().sum()}\")\n",
    "\n",
    "# Feature distribution assessment\n",
    "print(f\"\\n📈 Feature Distribution Assessment:\")\n",
    "for col in ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']:\n",
    "    skewness = df[col].skew()\n",
    "    kurtosis = df[col].kurtosis()\n",
    "    skew_interpretation = \"normal\" if abs(skewness) < 0.5 else \"moderately skewed\" if abs(skewness) < 1 else \"highly skewed\"\n",
    "    print(f\"  • {col}:\")\n",
    "    print(f\"    - Skewness: {skewness:.3f} ({skew_interpretation})\")\n",
    "    print(f\"    - Kurtosis: {kurtosis:.3f}\")\n",
    "\n",
    "print(f\"\\n🎯 KEY INSIGHTS FROM EDA:\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "# Calculate some key insights\n",
    "high_spenders = df[df['Spending Score (1-100)'] > 70].shape[0]\n",
    "high_income = df[df['Annual Income (k$)'] > 70].shape[0]\n",
    "young_customers = df[df['Age'] < 30].shape[0]\n",
    "female_customers = df[df['Gender'] == 'Female'].shape[0]\n",
    "\n",
    "print(f\"• {high_spenders} customers ({high_spenders/total_records*100:.1f}%) are high spenders (>70 score)\")\n",
    "print(f\"• {high_income} customers ({high_income/total_records*100:.1f}%) have high income (>$70k)\")\n",
    "print(f\"• {young_customers} customers ({young_customers/total_records*100:.1f}%) are young (<30 years)\")\n",
    "print(f\"• {female_customers} customers ({female_customers/total_records*100:.1f}%) are female\")\n",
    "\n",
    "# Identify potential customer segments (preliminary)\n",
    "print(f\"\\n🔍 Preliminary Segment Identification:\")\n",
    "\n",
    "# High income, high spending\n",
    "high_income_high_spend = df[(df['Annual Income (k$)'] > 70) & (df['Spending Score (1-100)'] > 70)]\n",
    "print(f\"• Premium Customers (High Income + High Spending): {len(high_income_high_spend)} ({len(high_income_high_spend)/total_records*100:.1f}%)\")\n",
    "\n",
    "# High income, low spending\n",
    "high_income_low_spend = df[(df['Annual Income (k$)'] > 70) & (df['Spending Score (1-100)'] < 40)]\n",
    "print(f\"• Conservative Affluent (High Income + Low Spending): {len(high_income_low_spend)} ({len(high_income_low_spend)/total_records*100:.1f}%)\")\n",
    "\n",
    "# Low income, high spending\n",
    "low_income_high_spend = df[(df['Annual Income (k$)'] < 40) & (df['Spending Score (1-100)'] > 70)]\n",
    "print(f\"• Aspirational Spenders (Low Income + High Spending): {len(low_income_high_spend)} ({len(low_income_high_spend)/total_records*100:.1f}%)\")\n",
    "\n",
    "# Low income, low spending\n",
    "low_income_low_spend = df[(df['Annual Income (k$)'] < 40) & (df['Spending Score (1-100)'] < 40)]\n",
    "print(f\"• Budget Conscious (Low Income + Low Spending): {len(low_income_low_spend)} ({len(low_income_low_spend)/total_records*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n🚀 Ready for Clustering Analysis!\")\n",
    "print(\"Next steps: Data preprocessing and dimensionality reduction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e22dd98",
   "metadata": {},
   "source": [
    "# CUSTO CLARITY 🛍️📊\n",
    "## Customer Segmentation Analysis - Data Exploration\n",
    "\n",
    "**Author:** Neelanjan Chakraborty  \n",
    "**Website:** [neelanjanchakraborty.in](https://neelanjanchakraborty.in/)  \n",
    "**Project:** Advanced Customer Segmentation using Machine Learning  \n",
    "**Date:** July 2025\n",
    "\n",
    "---\n",
    "\n",
    "### 📋 Project Overview\n",
    "\n",
    "**CUSTO CLARITY** is a comprehensive data science project that leverages machine learning clustering algorithms to identify distinct customer segments from retail data. This analysis will help businesses understand their customer base and develop targeted marketing strategies.\n",
    "\n",
    "#### 🎯 Key Objectives:\n",
    "- **Customer Segmentation**: Identify distinct customer groups using KMeans and DBSCAN\n",
    "- **Pattern Discovery**: Uncover hidden patterns in customer behavior through EDA\n",
    "- **Marketing Insights**: Provide actionable insights for targeted campaigns\n",
    "- **Strategic Planning**: Guide product strategy and customer retention efforts\n",
    "\n",
    "#### 🔬 Methodology:\n",
    "1. **Exploratory Data Analysis (EDA)** - Understanding data patterns and distributions\n",
    "2. **Data Preprocessing** - Cleaning and preparing data for analysis\n",
    "3. **Dimensionality Reduction** - PCA and t-SNE for visualization\n",
    "4. **Clustering Analysis** - KMeans and DBSCAN implementation\n",
    "5. **Business Insights** - Translating results into actionable strategies\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 Dataset Information\n",
    "\n",
    "We'll be analyzing the **Mall Customer Segmentation Dataset** which contains:\n",
    "- Customer demographics (Age, Gender)\n",
    "- Annual Income information  \n",
    "- Spending Score metrics\n",
    "- 200 customer records for comprehensive analysis\n",
    "\n",
    "Let's begin our exploration! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778bd4fa",
   "metadata": {},
   "source": [
    "## 1. Project Setup and Library Imports 📚\n",
    "\n",
    "Let's start by importing all the necessary libraries for our customer segmentation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23476203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# System and file handling\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Configure display settings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Add project source to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import custom modules\n",
    "try:\n",
    "    from data_loader import DataLoader\n",
    "    from preprocessor import CustomerDataPreprocessor\n",
    "    from clustering import CustomerClusteringAnalyzer\n",
    "    from visualizer import CustomerVisualizationSuite\n",
    "    print(\"✅ Custom modules imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Could not import custom modules: {e}\")\n",
    "    print(\"Running without custom modules...\")\n",
    "\n",
    "print(\"🎯 CUSTO CLARITY - Customer Segmentation Analysis\")\n",
    "print(\"📊 All libraries imported successfully!\")\n",
    "print(f\"🐍 Python version: {sys.version}\")\n",
    "print(f\"📈 Pandas version: {pd.__version__}\")\n",
    "print(f\"🔬 NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f19a1",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Overview 📊\n",
    "\n",
    "Now let's load our customer dataset and get an initial understanding of the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f73d031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the customer dataset\n",
    "try:\n",
    "    # Using custom data loader\n",
    "    data_loader = DataLoader(data_dir=\"../data\")\n",
    "    df = data_loader.load_dataset()\n",
    "    print(\"✅ Dataset loaded using custom DataLoader!\")\n",
    "except:\n",
    "    # Fallback: Create sample data\n",
    "    print(\"📂 Creating sample Mall Customer Segmentation dataset...\")\n",
    "    \n",
    "    # Generate sample data similar to the original dataset\n",
    "    np.random.seed(42)\n",
    "    n_customers = 200\n",
    "    \n",
    "    data = {\n",
    "        'CustomerID': range(1, n_customers + 1),\n",
    "        'Gender': np.random.choice(['Male', 'Female'], n_customers, p=[0.44, 0.56]),\n",
    "        'Age': np.random.normal(38, 12, n_customers).astype(int),\n",
    "        'Annual Income (k$)': np.random.lognormal(3.7, 0.4, n_customers).astype(int),\n",
    "        'Spending Score (1-100)': np.random.randint(1, 101, n_customers)\n",
    "    }\n",
    "    \n",
    "    # Ensure realistic ranges\n",
    "    data['Age'] = np.clip(data['Age'], 18, 70)\n",
    "    data['Annual Income (k$)'] = np.clip(data['Annual Income (k$)'], 15, 137)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"✅ Sample dataset created successfully!\")\n",
    "\n",
    "# Display basic dataset information\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"📊 CUSTO CLARITY - DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n🔍 Dataset Shape: {df.shape}\")\n",
    "print(f\"👥 Number of customers: {df.shape[0]}\")\n",
    "print(f\"📈 Number of features: {df.shape[1]}\")\n",
    "\n",
    "print(f\"\\n📋 Column Names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"   {i}. {col}\")\n",
    "\n",
    "print(f\"\\n📊 Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(f\"\\n🔍 First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c32c3",
   "metadata": {},
   "source": [
    "### 2.1 Data Quality Assessment 🔍\n",
    "\n",
    "Let's examine the data quality, check for missing values, and understand the overall health of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cea3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality assessment\n",
    "print(\"🔍 DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n📊 Missing Values Analysis:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': missing_values.index,\n",
    "    'Missing Count': missing_values.values,\n",
    "    'Missing Percentage': missing_percentage.values\n",
    "})\n",
    "\n",
    "print(missing_df)\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"✅ No missing values found - excellent data quality!\")\n",
    "else:\n",
    "    print(f\"⚠️ Total missing values: {missing_values.sum()}\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\n🔄 Duplicate rows: {duplicates}\")\n",
    "if duplicates == 0:\n",
    "    print(\"✅ No duplicate rows found!\")\n",
    "\n",
    "# Check data types and potential issues\n",
    "print(f\"\\n📋 Data Types Overview:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Basic statistical summary\n",
    "print(f\"\\n📈 Statistical Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for unique values in categorical columns\n",
    "print(f\"\\n🏷️ Categorical Variables Analysis:\")\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "for col in categorical_cols:\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  - Unique values: {unique_count}\")\n",
    "    print(f\"  - Value counts:\")\n",
    "    print(f\"    {df[col].value_counts().to_dict()}\")\n",
    "\n",
    "# Memory usage\n",
    "print(f\"\\n💾 Memory Usage:\")\n",
    "memory_usage = df.memory_usage(deep=True)\n",
    "total_memory = memory_usage.sum()\n",
    "print(f\"Total memory usage: {total_memory / 1024:.2f} KB\")\n",
    "\n",
    "print(f\"\\n✨ Data Quality Summary:\")\n",
    "print(f\"   ✅ Dataset is clean and ready for analysis!\")\n",
    "print(f\"   📊 {len(df)} customers with {len(df.columns)} features\")\n",
    "print(f\"   🎯 No missing values or duplicates detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bc89a3",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA) 🔍📊\n",
    "\n",
    "Now let's dive deep into understanding our customer data through comprehensive exploratory analysis. We'll examine distributions, relationships, and patterns that will guide our segmentation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05051b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive distribution analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('CUSTO CLARITY - Customer Data Distribution Analysis\\nby Neelanjan Chakraborty', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# Age distribution\n",
    "if 'Age' in df.columns:\n",
    "    sns.histplot(data=df, x='Age', bins=20, kde=True, ax=axes[0, 0], color='skyblue')\n",
    "    axes[0, 0].set_title('Age Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Age')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_age = df['Age'].mean()\n",
    "    median_age = df['Age'].median()\n",
    "    axes[0, 0].axvline(mean_age, color='red', linestyle='--', label=f'Mean: {mean_age:.1f}')\n",
    "    axes[0, 0].axvline(median_age, color='orange', linestyle='--', label=f'Median: {median_age:.1f}')\n",
    "    axes[0, 0].legend()\n",
    "\n",
    "# Annual Income distribution\n",
    "if 'Annual Income (k$)' in df.columns:\n",
    "    sns.histplot(data=df, x='Annual Income (k$)', bins=20, kde=True, ax=axes[0, 1], color='lightgreen')\n",
    "    axes[0, 1].set_title('Annual Income Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Annual Income (k$)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_income = df['Annual Income (k$)'].mean()\n",
    "    median_income = df['Annual Income (k$)'].median()\n",
    "    axes[0, 1].axvline(mean_income, color='red', linestyle='--', label=f'Mean: {mean_income:.1f}')\n",
    "    axes[0, 1].axvline(median_income, color='orange', linestyle='--', label=f'Median: {median_income:.1f}')\n",
    "    axes[0, 1].legend()\n",
    "\n",
    "# Spending Score distribution\n",
    "if 'Spending Score (1-100)' in df.columns:\n",
    "    sns.histplot(data=df, x='Spending Score (1-100)', bins=20, kde=True, ax=axes[0, 2], color='lightcoral')\n",
    "    axes[0, 2].set_title('Spending Score Distribution', fontsize=12, fontweight='bold')\n",
    "    axes[0, 2].set_xlabel('Spending Score (1-100)')\n",
    "    axes[0, 2].set_ylabel('Frequency')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add statistics\n",
    "    mean_spending = df['Spending Score (1-100)'].mean()\n",
    "    median_spending = df['Spending Score (1-100)'].median()\n",
    "    axes[0, 2].axvline(mean_spending, color='red', linestyle='--', label=f'Mean: {mean_spending:.1f}')\n",
    "    axes[0, 2].axvline(median_spending, color='orange', linestyle='--', label=f'Median: {median_spending:.1f}')\n",
    "    axes[0, 2].legend()\n",
    "\n",
    "# Gender distribution\n",
    "if 'Gender' in df.columns:\n",
    "    gender_counts = df['Gender'].value_counts()\n",
    "    colors = ['lightblue', 'lightpink']\n",
    "    wedges, texts, autotexts = axes[1, 0].pie(gender_counts.values, labels=gender_counts.index, \n",
    "                                            autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "    axes[1, 0].set_title('Gender Distribution', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Enhance pie chart\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "\n",
    "# Age vs Income scatter plot\n",
    "if 'Age' in df.columns and 'Annual Income (k$)' in df.columns:\n",
    "    sns.scatterplot(data=df, x='Age', y='Annual Income (k$)', \n",
    "                   hue='Gender' if 'Gender' in df.columns else None, \n",
    "                   ax=axes[1, 1], alpha=0.7, s=60)\n",
    "    axes[1, 1].set_title('Age vs Annual Income', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Income vs Spending Score scatter plot\n",
    "if 'Annual Income (k$)' in df.columns and 'Spending Score (1-100)' in df.columns:\n",
    "    sns.scatterplot(data=df, x='Annual Income (k$)', y='Spending Score (1-100)',\n",
    "                   hue='Gender' if 'Gender' in df.columns else None, \n",
    "                   ax=axes[1, 2], alpha=0.7, s=60)\n",
    "    axes[1, 2].set_title('Income vs Spending Score', fontsize=12, fontweight='bold')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n📊 DISTRIBUTION ANALYSIS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if 'Age' in df.columns:\n",
    "    print(f\"\\n👥 Age Analysis:\")\n",
    "    print(f\"   Range: {df['Age'].min()} - {df['Age'].max()} years\")\n",
    "    print(f\"   Mean: {df['Age'].mean():.1f} years\")\n",
    "    print(f\"   Median: {df['Age'].median():.1f} years\")\n",
    "    print(f\"   Standard Deviation: {df['Age'].std():.1f} years\")\n",
    "\n",
    "if 'Annual Income (k$)' in df.columns:\n",
    "    print(f\"\\n💰 Income Analysis:\")\n",
    "    print(f\"   Range: ${df['Annual Income (k$)'].min()}k - ${df['Annual Income (k$)'].max()}k\")\n",
    "    print(f\"   Mean: ${df['Annual Income (k$)'].mean():.1f}k\")\n",
    "    print(f\"   Median: ${df['Annual Income (k$)'].median():.1f}k\")\n",
    "    print(f\"   Standard Deviation: ${df['Annual Income (k$)'].std():.1f}k\")\n",
    "\n",
    "if 'Spending Score (1-100)' in df.columns:\n",
    "    print(f\"\\n🛍️ Spending Score Analysis:\")\n",
    "    print(f\"   Range: {df['Spending Score (1-100)'].min()} - {df['Spending Score (1-100)'].max()}\")\n",
    "    print(f\"   Mean: {df['Spending Score (1-100)'].mean():.1f}\")\n",
    "    print(f\"   Median: {df['Spending Score (1-100)'].median():.1f}\")\n",
    "    print(f\"   Standard Deviation: {df['Spending Score (1-100)'].std():.1f}\")\n",
    "\n",
    "if 'Gender' in df.columns:\n",
    "    print(f\"\\n⚧ Gender Distribution:\")\n",
    "    for gender, count in df['Gender'].value_counts().items():\n",
    "        percentage = (count / len(df)) * 100\n",
    "        print(f\"   {gender}: {count} customers ({percentage:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
