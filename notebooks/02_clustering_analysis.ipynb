{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddbb2f89",
   "metadata": {},
   "source": [
    "# CUSTO CLARITY - Customer Segmentation Analysis\n",
    "## 02. Clustering Analysis & Customer Segmentation\n",
    "\n",
    "**Author**: Neelanjan Chakraborty  \n",
    "**Website**: [neelanjanchakraborty.in](https://neelanjanchakraborty.in/)  \n",
    "**Project**: Customer Segmentation for Retail Strategy  \n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“‹ Notebook Overview\n",
    "\n",
    "This notebook implements comprehensive clustering analysis using multiple algorithms including K-Means, DBSCAN, and Hierarchical clustering. We'll also apply dimensionality reduction techniques (PCA, t-SNE) and evaluate cluster quality.\n",
    "\n",
    "### ğŸ¯ Objectives\n",
    "- Preprocess data for clustering analysis\n",
    "- Apply dimensionality reduction techniques\n",
    "- Implement multiple clustering algorithms\n",
    "- Evaluate and compare clustering results\n",
    "- Generate business insights from customer segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae277d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append(os.path.join(os.getcwd(), '..', 'src'))\n",
    "\n",
    "# Import custom modules\n",
    "from data_loader import DataLoader\n",
    "from preprocessor import CustomerDataPreprocessor\n",
    "from clustering import CustomerClusteringAnalyzer\n",
    "from visualizer import CustomerVisualizationSuite\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(\"ğŸš€ Ready for Clustering Analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3f25d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Preprocess Data\n",
    "print(\"ğŸ“Š LOADING AND PREPROCESSING DATA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Load dataset\n",
    "loader = DataLoader()\n",
    "df = loader.load_dataset()\n",
    "\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = CustomerDataPreprocessor()\n",
    "\n",
    "# Perform complete preprocessing\n",
    "df_processed, X = preprocessor.prepare_for_clustering(\n",
    "    df, \n",
    "    target_columns=['Age', 'Annual Income (k$)', 'Spending Score (1-100)'],\n",
    "    scaling_method='standard'\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Preprocessing completed!\")\n",
    "print(f\"Processed dataset shape: {df_processed.shape}\")\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Features used: {preprocessor.feature_names}\")\n",
    "\n",
    "# Display preprocessing summary\n",
    "summary = preprocessor.get_preprocessing_summary()\n",
    "print(f\"\\nğŸ“‹ Preprocessing Summary:\")\n",
    "for step in summary['steps_performed']:\n",
    "    print(f\"  â€¢ {step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4369397c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction Analysis\n",
    "print(\"ğŸ“‰ DIMENSIONALITY REDUCTION ANALYSIS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Initialize clustering analyzer\n",
    "analyzer = CustomerClusteringAnalyzer()\n",
    "\n",
    "# Apply dimensionality reduction\n",
    "reduction_results = analyzer.apply_dimensionality_reduction(X, methods=['pca', 'tsne'])\n",
    "\n",
    "# PCA Analysis\n",
    "if 'pca' in reduction_results:\n",
    "    pca_data = reduction_results['pca']['data']\n",
    "    pca_model = reduction_results['pca']['model']\n",
    "    explained_variance = reduction_results['pca']['explained_variance_ratio']\n",
    "    total_variance = reduction_results['pca']['total_variance_explained']\n",
    "    \n",
    "    print(f\"ğŸ” PCA Results:\")\n",
    "    print(f\"  â€¢ PC1 Explained Variance: {explained_variance[0]:.3f} ({explained_variance[0]*100:.1f}%)\")\n",
    "    print(f\"  â€¢ PC2 Explained Variance: {explained_variance[1]:.3f} ({explained_variance[1]*100:.1f}%)\")\n",
    "    print(f\"  â€¢ Total Variance Explained: {total_variance:.3f} ({total_variance*100:.1f}%)\")\n",
    "    \n",
    "    # Show PCA components\n",
    "    feature_names = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n",
    "    components_df = pd.DataFrame(\n",
    "        pca_model.components_,\n",
    "        columns=feature_names,\n",
    "        index=['PC1', 'PC2']\n",
    "    )\n",
    "    print(f\"\\nğŸ“Š PCA Components:\")\n",
    "    print(components_df.round(3))\n",
    "\n",
    "# t-SNE Analysis\n",
    "if 'tsne' in reduction_results:\n",
    "    tsne_data = reduction_results['tsne']['data']\n",
    "    print(f\"\\nğŸ” t-SNE Results:\")\n",
    "    print(f\"  â€¢ t-SNE embedding shape: {tsne_data.shape}\")\n",
    "    print(f\"  â€¢ Applied for non-linear dimensionality reduction\")\n",
    "\n",
    "print(f\"\\nâœ… Dimensionality reduction completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2ca11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Cluster Analysis - Elbow Method & Silhouette Analysis\n",
    "print(\"ğŸ¯ FINDING OPTIMAL NUMBER OF CLUSTERS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Find optimal clusters\n",
    "optimal_results = analyzer.find_optimal_clusters_kmeans(X, max_clusters=10)\n",
    "\n",
    "print(f\"ğŸ“Š Cluster Optimization Results:\")\n",
    "print(f\"  â€¢ Elbow Method Optimal k: {optimal_results['elbow_optimal_k']}\")\n",
    "print(f\"  â€¢ Silhouette Analysis Optimal k: {optimal_results['silhouette_optimal_k']}\")\n",
    "print(f\"  â€¢ Recommended k: {optimal_results['recommended_k']}\")\n",
    "\n",
    "# Initialize visualization suite\n",
    "viz_suite = CustomerVisualizationSuite()\n",
    "\n",
    "# Create elbow and silhouette plots\n",
    "viz_suite.plot_elbow_analysis(\n",
    "    optimal_results['k_range'],\n",
    "    optimal_results['inertias'],\n",
    "    optimal_results['silhouette_scores']\n",
    ")\n",
    "\n",
    "# Display detailed metrics for each k\n",
    "print(f\"\\nğŸ“ˆ Detailed Metrics by Number of Clusters:\")\n",
    "metrics_df = pd.DataFrame({\n",
    "    'k': optimal_results['k_range'],\n",
    "    'Inertia': optimal_results['inertias'],\n",
    "    'Silhouette Score': optimal_results['silhouette_scores'],\n",
    "    'Calinski-Harabasz': optimal_results['calinski_harabasz_scores'],\n",
    "    'Davies-Bouldin': optimal_results['davies_bouldin_scores']\n",
    "})\n",
    "print(metrics_df.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04e1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Clustering Analysis\n",
    "print(\"ğŸ” K-MEANS CLUSTERING ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Perform K-Means clustering with optimal k\n",
    "optimal_k = optimal_results['recommended_k']\n",
    "kmeans_results = analyzer.perform_kmeans_clustering(X, n_clusters=optimal_k)\n",
    "\n",
    "print(f\"ğŸ“Š K-Means Results (k={optimal_k}):\")\n",
    "print(f\"  â€¢ Silhouette Score: {kmeans_results['silhouette_score']:.3f}\")\n",
    "print(f\"  â€¢ Calinski-Harabasz Score: {kmeans_results['calinski_harabasz_score']:.3f}\")\n",
    "print(f\"  â€¢ Davies-Bouldin Score: {kmeans_results['davies_bouldin_score']:.3f}\")\n",
    "print(f\"  â€¢ Inertia: {kmeans_results['inertia']:.3f}\")\n",
    "\n",
    "# Get cluster labels\n",
    "kmeans_labels = kmeans_results['labels']\n",
    "unique_labels = np.unique(kmeans_labels)\n",
    "\n",
    "print(f\"\\nğŸ·ï¸ Cluster Distribution:\")\n",
    "cluster_counts = pd.Series(kmeans_labels).value_counts().sort_index()\n",
    "for cluster, count in cluster_counts.items():\n",
    "    percentage = count / len(kmeans_labels) * 100\n",
    "    print(f\"  â€¢ Cluster {cluster}: {count} customers ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize K-Means clusters\n",
    "feature_names = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n",
    "\n",
    "# 2D visualization using original features\n",
    "viz_suite.plot_cluster_analysis_2d(\n",
    "    X[:, [1, 2]],  # Income and Spending Score\n",
    "    kmeans_labels,\n",
    "    title=\"K-Means Customer Segmentation (Income vs Spending)\",\n",
    "    feature_names=['Annual Income (k$)', 'Spending Score (1-100)']\n",
    ")\n",
    "\n",
    "# 3D visualization\n",
    "viz_suite.plot_cluster_analysis_3d(\n",
    "    X,\n",
    "    kmeans_labels,\n",
    "    title=\"K-Means Customer Segmentation 3D\",\n",
    "    feature_names=feature_names\n",
    ")\n",
    "\n",
    "print(f\"âœ… K-Means clustering visualization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088a5b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN Clustering Analysis\n",
    "print(\"ğŸ” DBSCAN CLUSTERING ANALYSIS\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "# Optimize DBSCAN parameters\n",
    "print(\"âš™ï¸ Optimizing DBSCAN Parameters...\")\n",
    "dbscan_optimization = analyzer.optimize_dbscan_parameters(X)\n",
    "\n",
    "if dbscan_optimization['best_params']:\n",
    "    best_eps = dbscan_optimization['best_params']['eps']\n",
    "    best_min_samples = dbscan_optimization['best_params']['min_samples']\n",
    "    print(f\"  â€¢ Best eps: {best_eps}\")\n",
    "    print(f\"  â€¢ Best min_samples: {best_min_samples}\")\n",
    "    print(f\"  â€¢ Best silhouette score: {dbscan_optimization['best_score']:.3f}\")\n",
    "    \n",
    "    # Perform DBSCAN with optimized parameters\n",
    "    dbscan_results = analyzer.perform_dbscan_clustering(X, eps=best_eps, min_samples=best_min_samples)\n",
    "else:\n",
    "    print(\"  â€¢ Using default parameters\")\n",
    "    dbscan_results = analyzer.perform_dbscan_clustering(X)\n",
    "\n",
    "print(f\"\\nğŸ“Š DBSCAN Results:\")\n",
    "print(f\"  â€¢ Number of Clusters: {dbscan_results['n_clusters']}\")\n",
    "print(f\"  â€¢ Number of Noise Points: {dbscan_results['n_noise']}\")\n",
    "print(f\"  â€¢ Noise Percentage: {dbscan_results['n_noise']/len(X)*100:.1f}%\")\n",
    "if dbscan_results['silhouette_score']:\n",
    "    print(f\"  â€¢ Silhouette Score: {dbscan_results['silhouette_score']:.3f}\")\n",
    "\n",
    "# Get DBSCAN labels\n",
    "dbscan_labels = dbscan_results['labels']\n",
    "\n",
    "if dbscan_results['n_clusters'] > 0:\n",
    "    print(f\"\\nğŸ·ï¸ DBSCAN Cluster Distribution:\")\n",
    "    dbscan_cluster_counts = pd.Series(dbscan_labels).value_counts().sort_index()\n",
    "    for cluster, count in dbscan_cluster_counts.items():\n",
    "        percentage = count / len(dbscan_labels) * 100\n",
    "        cluster_name = \"Noise\" if cluster == -1 else f\"Cluster {cluster}\"\n",
    "        print(f\"  â€¢ {cluster_name}: {count} customers ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Visualize DBSCAN clusters if valid clusters found\n",
    "    viz_suite.plot_cluster_analysis_2d(\n",
    "        X[:, [1, 2]],  # Income and Spending Score\n",
    "        dbscan_labels,\n",
    "        title=\"DBSCAN Customer Segmentation (Income vs Spending)\",\n",
    "        feature_names=['Annual Income (k$)', 'Spending Score (1-100)']\n",
    "    )\n",
    "else:\n",
    "    print(\"  âš ï¸ DBSCAN did not find meaningful clusters with current parameters\")\n",
    "\n",
    "print(f\"âœ… DBSCAN clustering analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f7fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Clustering Analysis\n",
    "print(\"ğŸ” HIERARCHICAL CLUSTERING ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "hierarchical_results = analyzer.perform_hierarchical_clustering(X, n_clusters=optimal_k)\n",
    "\n",
    "print(f\"ğŸ“Š Hierarchical Clustering Results (k={optimal_k}):\")\n",
    "print(f\"  â€¢ Silhouette Score: {hierarchical_results['silhouette_score']:.3f}\")\n",
    "print(f\"  â€¢ Calinski-Harabasz Score: {hierarchical_results['calinski_harabasz_score']:.3f}\")\n",
    "print(f\"  â€¢ Davies-Bouldin Score: {hierarchical_results['davies_bouldin_score']:.3f}\")\n",
    "print(f\"  â€¢ Linkage Method: {hierarchical_results['linkage']}\")\n",
    "\n",
    "# Get hierarchical labels\n",
    "hierarchical_labels = hierarchical_results['labels']\n",
    "\n",
    "print(f\"\\nğŸ·ï¸ Hierarchical Cluster Distribution:\")\n",
    "hierarchical_cluster_counts = pd.Series(hierarchical_labels).value_counts().sort_index()\n",
    "for cluster, count in hierarchical_cluster_counts.items():\n",
    "    percentage = count / len(hierarchical_labels) * 100\n",
    "    print(f\"  â€¢ Cluster {cluster}: {count} customers ({percentage:.1f}%)\")\n",
    "\n",
    "# Visualize hierarchical clusters\n",
    "viz_suite.plot_cluster_analysis_2d(\n",
    "    X[:, [1, 2]],  # Income and Spending Score\n",
    "    hierarchical_labels,\n",
    "    title=\"Hierarchical Customer Segmentation (Income vs Spending)\",\n",
    "    feature_names=['Annual Income (k$)', 'Spending Score (1-100)']\n",
    ")\n",
    "\n",
    "print(f\"âœ… Hierarchical clustering analysis completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0ae331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Clustering Methods\n",
    "print(\"âš–ï¸ CLUSTERING METHODS COMPARISON\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Compare all clustering methods\n",
    "comparison_df = analyzer.compare_clustering_methods(X)\n",
    "\n",
    "print(\"ğŸ“Š Clustering Methods Performance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Determine best method based on silhouette score\n",
    "valid_scores = comparison_df.dropna(subset=['Silhouette Score'])\n",
    "if not valid_scores.empty:\n",
    "    best_method = valid_scores.loc[valid_scores['Silhouette Score'].idxmax(), 'Method']\n",
    "    best_score = valid_scores['Silhouette Score'].max()\n",
    "    print(f\"\\nğŸ† Best Performing Method: {best_method} (Silhouette Score: {best_score:.3f})\")\n",
    "    \n",
    "    # Select best clustering results for further analysis\n",
    "    if best_method == 'K-Means':\n",
    "        best_labels = kmeans_labels\n",
    "        best_results = kmeans_results\n",
    "    elif best_method == 'DBSCAN':\n",
    "        best_labels = dbscan_labels\n",
    "        best_results = dbscan_results\n",
    "    else:\n",
    "        best_labels = hierarchical_labels\n",
    "        best_results = hierarchical_results\n",
    "    \n",
    "    print(f\"ğŸ¯ Using {best_method} results for business analysis\")\n",
    "else:\n",
    "    # Fallback to K-Means if no valid scores\n",
    "    best_labels = kmeans_labels\n",
    "    best_results = kmeans_results\n",
    "    best_method = 'K-Means'\n",
    "    print(f\"ğŸ¯ Using K-Means results for business analysis (fallback)\")\n",
    "\n",
    "print(f\"âœ… Clustering comparison completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc9543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction Visualization\n",
    "print(\"ğŸ“Š DIMENSIONALITY REDUCTION VISUALIZATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Create visualization with best clustering results\n",
    "reduced_data = {\n",
    "    'pca': reduction_results['pca']['data'],\n",
    "    'tsne': reduction_results['tsne']['data']\n",
    "}\n",
    "\n",
    "viz_suite.plot_dimensionality_reduction(\n",
    "    X, reduced_data, best_labels\n",
    ")\n",
    "\n",
    "print(f\"âœ… Dimensionality reduction visualization completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daedef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Cluster Profiling\n",
    "print(\"ğŸ‘¥ CUSTOMER CLUSTER PROFILING\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "# Analyze cluster profiles\n",
    "feature_names = ['Age', 'Annual Income (k$)', 'Spending Score (1-100)']\n",
    "profiles = analyzer.analyze_cluster_profiles(df, best_labels, feature_names)\n",
    "\n",
    "# Display numeric profiles\n",
    "if 'numeric' in profiles:\n",
    "    print(\"ğŸ“Š Cluster Profiles (Numeric Features):\")\n",
    "    print(profiles['numeric'].round(2))\n",
    "\n",
    "# Display cluster sizes\n",
    "print(f\"\\nğŸ“ˆ Cluster Sizes:\")\n",
    "cluster_sizes = profiles['cluster_sizes']\n",
    "cluster_percentages = profiles['cluster_percentages']\n",
    "for cluster in cluster_sizes.index:\n",
    "    print(f\"  â€¢ Cluster {cluster}: {cluster_sizes[cluster]} customers ({cluster_percentages[cluster]:.1f}%)\")\n",
    "\n",
    "# Visualize cluster profiles\n",
    "viz_suite.plot_cluster_profiles(profiles)\n",
    "viz_suite.plot_cluster_sizes(cluster_sizes)\n",
    "\n",
    "print(f\"âœ… Cluster profiling completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667b70f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Business Insights\n",
    "print(\"ğŸ’¼ BUSINESS INSIGHTS GENERATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Generate business insights\n",
    "business_insights = analyzer.generate_business_insights(profiles, best_labels)\n",
    "\n",
    "print(f\"ğŸ¯ Customer Segment Analysis & Business Recommendations:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for cluster_key, insight in business_insights.items():\n",
    "    print(f\"\\n{insight}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Create interactive business dashboard\n",
    "print(f\"\\nğŸ“ˆ Creating Interactive Business Dashboard...\")\n",
    "viz_suite.create_business_dashboard(df, best_labels, profiles)\n",
    "\n",
    "print(f\"âœ… Business insights generation completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1e78e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results and Generate Summary Report\n",
    "print(\"ğŸ’¾ SAVING RESULTS AND GENERATING SUMMARY\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Add cluster labels to original dataframe\n",
    "df_final = df.copy()\n",
    "df_final['Cluster'] = best_labels\n",
    "df_final['Cluster_Method'] = best_method\n",
    "\n",
    "# Save processed data\n",
    "os.makedirs('../outputs', exist_ok=True)\n",
    "df_final.to_csv('../outputs/customer_segments.csv', index=False)\n",
    "print(f\"âœ… Customer segments saved to '../outputs/customer_segments.csv'\")\n",
    "\n",
    "# Generate comprehensive summary\n",
    "print(f\"\\nğŸ“‹ CUSTO CLARITY - CLUSTERING ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“Š Dataset: {df.shape[0]} customers with {df.shape[1]} features\")\n",
    "print(f\"ğŸ” Best Clustering Method: {best_method}\")\n",
    "print(f\"ğŸ¯ Number of Segments: {len(np.unique(best_labels))}\")\n",
    "if best_method in ['K-Means', 'Hierarchical']:\n",
    "    print(f\"ğŸ“ˆ Silhouette Score: {best_results['silhouette_score']:.3f}\")\n",
    "    print(f\"ğŸ“ˆ Calinski-Harabasz Score: {best_results['calinski_harabasz_score']:.3f}\")\n",
    "    print(f\"ğŸ“ˆ Davies-Bouldin Score: {best_results['davies_bouldin_score']:.3f}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Key Business Recommendations:\")\n",
    "print(f\"  â€¢ Implement targeted marketing strategies for each segment\")\n",
    "print(f\"  â€¢ Develop personalized product offerings\")\n",
    "print(f\"  â€¢ Focus on high-value customer retention\")\n",
    "print(f\"  â€¢ Create segment-specific promotional campaigns\")\n",
    "\n",
    "print(f\"\\nğŸ”® Next Steps:\")\n",
    "print(f\"  â€¢ Validate clusters with domain experts\")\n",
    "print(f\"  â€¢ Implement A/B testing for segment strategies\")\n",
    "print(f\"  â€¢ Monitor cluster stability over time\")\n",
    "print(f\"  â€¢ Develop automated segmentation pipeline\")\n",
    "\n",
    "print(f\"\\nğŸš€ Clustering Analysis Completed Successfully!\")\n",
    "print(f\"\\nğŸ‘¨â€ğŸ’» Analysis by: Neelanjan Chakraborty\")\n",
    "print(f\"ğŸŒ Website: https://neelanjanchakraborty.in/\")\n",
    "print(f\"ğŸ“§ Contact: Available via website\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
